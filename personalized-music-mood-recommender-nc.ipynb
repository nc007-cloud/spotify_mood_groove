{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10049,"sourceType":"datasetVersion","datasetId":6985},{"sourceId":2936818,"sourceType":"datasetVersion","datasetId":1800580},{"sourceId":3995285,"sourceType":"datasetVersion","datasetId":2370674},{"sourceId":3949876,"sourceType":"kernelVersion"},{"sourceId":54619392,"sourceType":"kernelVersion"},{"sourceId":135900347,"sourceType":"kernelVersion"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nilovnachatterjee/personalized-music-mood-recommender-nc?scriptVersionId=223304950\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Building Mood Based Music Recommendation System using Spotify dataset and the Million Song Dataset\nimage.png","metadata":{"execution":{"iopub.status.busy":"2025-02-16T05:14:51.550232Z","iopub.execute_input":"2025-02-16T05:14:51.55064Z","iopub.status.idle":"2025-02-16T05:14:51.558598Z","shell.execute_reply.started":"2025-02-16T05:14:51.550605Z","shell.execute_reply":"2025-02-16T05:14:51.556441Z"}}},{"cell_type":"markdown","source":"# =========================================\n# 1. Introduction\n# =========================================\n\"\"\"\nIn this notebook, a Mood-Based Music Recommendation System was built using \nthe Spotify Dataset and the Million Song Dataset. The following steps were adopted:\n\n1. Load and explore the data.\n2. Clean and preprocess the data (handle duplicates, missing values, outliers).\n3. Perform exploratory data analysis (EDA).\n4. Cluster songs based on audio features (K-Means).\n5. Build and evaluate a mood classification model.\n6. Create a neural network for recommendation based on mood clusters.\n\nA separate Word/PDF document will summarize the key findings and link back\nto this notebook.\n\"\"\"","metadata":{}},{"cell_type":"code","source":"# =========================================\n# 2. Import Libraries\n# =========================================\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nimport json\nimport nbformat\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Web scraping / HTML parsing\nfrom bs4 import BeautifulSoup \nfrom io import StringIO \n\n\n# Preprocessing & Feature Engineering\nfrom imblearn.over_sampling import RandomOverSampler\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, RobustScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score\n\n#Deep Learning\nfrom tensorflow.keras.models import Sequential\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:12:43.787389Z","iopub.execute_input":"2025-02-19T04:12:43.787842Z","iopub.status.idle":"2025-02-19T04:12:43.797307Z","shell.execute_reply.started":"2025-02-19T04:12:43.787809Z","shell.execute_reply":"2025-02-19T04:12:43.796124Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:12:48.415014Z","iopub.execute_input":"2025-02-19T04:12:48.41539Z","iopub.status.idle":"2025-02-19T04:12:48.440409Z","shell.execute_reply.started":"2025-02-19T04:12:48.415363Z","shell.execute_reply":"2025-02-19T04:12:48.439167Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/spotify-song-prediction-and-recommendation-system/__results__.html\n/kaggle/input/spotify-song-prediction-and-recommendation-system/__notebook_source__.ipynb\n/kaggle/input/spotify-song-prediction-and-recommendation-system/__notebook__.ipynb\n/kaggle/input/spotify-song-prediction-and-recommendation-system/__output__.json\n/kaggle/input/spotify-song-prediction-and-recommendation-system/custom.css\n/kaggle/input/million-song-dataset/Testing_set_songs.csv\n/kaggle/input/million-song-dataset/Training_set_songs.csv\n/kaggle/input/million-song-dataset-studies/song_data1.csv\n/kaggle/input/million-song-dataset-studies/song_data4.csv\n/kaggle/input/million-song-dataset-studies/song_data2.csv\n/kaggle/input/million-song-dataset-studies/song_data3.csv\n/kaggle/input/million-song-dataset-studies/song_data7.csv\n/kaggle/input/million-song-dataset-studies/song_data6.csv\n/kaggle/input/million-song-dataset-studies/song_data5.csv\n/kaggle/input/spotify-user-behavior-analysis/__results__.html\n/kaggle/input/spotify-user-behavior-analysis/__notebook_source__.ipynb\n/kaggle/input/spotify-user-behavior-analysis/__resultx__.html\n/kaggle/input/spotify-user-behavior-analysis/__notebook__.ipynb\n/kaggle/input/spotify-user-behavior-analysis/__output__.json\n/kaggle/input/spotify-user-behavior-analysis/custom.css\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___8_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___76_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___11_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___9_1.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___14_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___12_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___10_1.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___7_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___13_0.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___17_1.png\n/kaggle/input/spotify-user-behavior-analysis/__results___files/__results___74_0.png\n/kaggle/input/musics-depending-on-demographic-data/__results__.html\n/kaggle/input/musics-depending-on-demographic-data/__output__.json\n/kaggle/input/musics-depending-on-demographic-data/custom.css\n/kaggle/input/musics-depending-on-demographic-data/__results___files/__results___6_0.png\n/kaggle/input/spotify-dataset/data/data_by_year.csv\n/kaggle/input/spotify-dataset/data/data_by_genres.csv\n/kaggle/input/spotify-dataset/data/data_w_genres.csv\n/kaggle/input/spotify-dataset/data/data_by_artist.csv\n/kaggle/input/spotify-dataset/data/data.csv\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"data_dirs = {\n    \"million_song\" : \"/kaggle/input/million-song-dataset\",\n    \"spotify_songs\": \"/kaggle/input/spotify-dataset\",\n    \"million_song_studies\": \"/kaggle/input/million-song-dataset-studies\",\n    \"spotify_user_behavior\": \"/kaggle/input/spotify-user-behavior-analysis\",  # Integrated the Spotify User Behavior Analysis\n    \"spotify_song_prediction\": \"/kaggle/input/spotify-song-prediction-and-recommendation-system\",\n    \"musics_demographic_data\": \"/kaggle/input/musics-depending-on-demographic-data\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:12:51.621858Z","iopub.execute_input":"2025-02-19T04:12:51.622251Z","iopub.status.idle":"2025-02-19T04:12:51.626745Z","shell.execute_reply.started":"2025-02-19T04:12:51.622219Z","shell.execute_reply":"2025-02-19T04:12:51.625641Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# =========================================\n# 3. Helper Functions\n# =========================================\ndef extract_tables_from_html(html_path: str):\n    \"\"\"\n    Extracts all tables from an HTML file and returns them as a list of DataFrames.\n    \"\"\"\n    try:\n        with open(html_path, 'r', encoding='utf-8') as file:\n            soup = BeautifulSoup(file, 'html.parser')\n        tables = pd.read_html(StringIO(str(soup)))\n        return tables\n    except Exception as e:\n        print(f\"Error reading HTML file: {e}\")\n        return None\n\ndef list_files(directory: str):\n    \"\"\"\n    Lists all files in a given directory. Returns a list of file names.\n    \"\"\"\n    try:\n        files = os.listdir(directory)\n        print(f\"Files in {directory}:\", files)\n        return files\n    except Exception as e:\n        print(f\"Error listing files in {directory}: {e}\")\n        return []\n\ndef load_specific_datasets(data_dirs: dict):\n    \"\"\"\n    Loads multiple datasets (Million Song, Spotify, etc.) from specified directories.\n    Returns a dictionary of DataFrames or dict-of-DataFrames.\n    \"\"\"\n    datasets = {}\n    # Example usage of listing and loading. Adjust to your actual files.\n    for name, path in data_dirs.items():\n        print(f\"Loading dataset from: {name} -> {path}\")\n        # Example: If you have CSVs in that folder, you can load them\n        files = list_files(path)\n        # Add your custom loading logic per dataset name...\n        # (Skipping the full logic here for brevity.)\n    return datasets\n\ndef handle_missing_values(df: pd.DataFrame):\n    \"\"\"\n    Handles missing values by filling numerical columns with mean\n    and categorical columns with mode. Returns a cleaned copy of the DataFrame.\n    \"\"\"\n    if df is None:\n        return None\n\n    df = df.copy()\n    # Fill numerical columns with mean, categorical with mode\n    for col in df.columns:\n        if df[col].dtype in ['float64', 'int64']:\n            df[col].fillna(df[col].mean(), inplace=True)\n        else:\n            df[col].fillna(df[col].mode()[0], inplace=True)\n    return df\n\ndef remove_duplicates(df: pd.DataFrame):\n    \"\"\"\n    Removes duplicate rows from a DataFrame.\n    \"\"\"\n    if df is None:\n        return None\n    df = df.drop_duplicates().reset_index(drop=True)\n    return df\n\ndef detect_and_remove_outliers(df: pd.DataFrame, numeric_cols: list, iqr_multiplier: float = 1.5):\n    \"\"\"\n    Detects and removes outliers based on the IQR rule.\n    Returns a DataFrame without outliers in the specified numeric_cols.\n    \"\"\"\n    df_clean = df.copy()\n    for col in numeric_cols:\n        if col in df_clean.columns and df_clean[col].dtype in [np.float64, np.int64]:\n            Q1 = df_clean[col].quantile(0.25)\n            Q3 = df_clean[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - iqr_multiplier * IQR\n            upper_bound = Q3 + iqr_multiplier * IQR\n            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n    df_clean.reset_index(drop=True, inplace=True)\n    return df_clean\n\ndef encode_categorical_features(df: pd.DataFrame):\n    \"\"\"\n    Encodes categorical columns in a DataFrame using LabelEncoder.\n    Returns the transformed DataFrame and a dictionary of encoders.\n    \"\"\"\n    if df is None:\n        return None, {}\n\n    df = df.copy()\n    label_encoders = {}\n    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n    for col in categorical_columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col].astype(str))\n        label_encoders[col] = le\n    return df, label_encoders\n\ndef combine_numerical_data(datasets: dict) -> pd.DataFrame:\n    \"\"\"\n    Combines numerical data from multiple DataFrames into a single DataFrame.\n    Works if your 'datasets' dict has DataFrames or dict-of-DataFrames.\n    \"\"\"\n    combined_data = pd.DataFrame()\n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n    return combined_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:12:52.205657Z","iopub.execute_input":"2025-02-19T04:12:52.205981Z","iopub.status.idle":"2025-02-19T04:12:52.221407Z","shell.execute_reply.started":"2025-02-19T04:12:52.205959Z","shell.execute_reply":"2025-02-19T04:12:52.220199Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## [2] Helper Functions\n\nBelow, we define **reusable helper functions** for:\n- **Extracting tables** from HTML files,\n- **Listing files** in a directory,\n- **Loading** multiple datasets,\n- **Handling missing values**,\n- **Removing duplicates**,\n- **Outlier detection**,\n- **Encoding categorical features**,\n- **Combining** data from multiple DataFrames.\n","metadata":{}},{"cell_type":"code","source":"# Function to extract tables from HTML files\n# =========================================\n# 3. Helper Functions\n# =========================================\ndef extract_tables_from_html(html_path):\n    try:\n        with open(html_path, 'r', encoding='utf-8') as file:\n            soup = BeautifulSoup(file, 'html.parser')\n\n        # Extract all tables from the HTML using StringIO to avoid FutureWarning\n        tables = pd.read_html(StringIO(str(soup)))\n        return tables\n    except Exception as e:\n        print(f\"Error reading HTML file: {e}\")\n        return None\n\n# Directories for datasets\ndata_dirs = {\n    \"spotify_user_behavior\": \"/kaggle/input/spotify-user-behavior-analysis\",\n    \"spotify_song_prediction\": \"/kaggle/input/spotify-song-prediction-and-recommendation-system\",\n    \"musics_demographic_data\": \"/kaggle/input/musics-depending-on-demographic-data\"\n}\n\n# Attempting to extract data from HTML files\nspotify_html_data = extract_tables_from_html(os.path.join(data_dirs[\"spotify_user_behavior\"], \"__results__.html\"))\nmusics_html_data = extract_tables_from_html(os.path.join(data_dirs[\"musics_demographic_data\"], \"__results__.html\"))\n\n# Display the first few rows of the extracted tables if available\nif spotify_html_data:\n    print(\"\\nSpotify User Behavior Data Sample:\")\n    print(spotify_html_data[0].head())  # Display the first table\nelse:\n    print(\"No data found in Spotify User Behavior HTML.\")\n\nif musics_html_data:\n    print(\"\\nMusics Demographic Data Sample:\")\n    print(musics_html_data[0].head())  # Display the first table\nelse:\n    print(\"No data found in Musics Demographic HTML.\")\n\n\ndef list_files(directory):\n    try:\n        files = os.listdir(directory)\n        print(f\"Files in {directory}:\", files)\n        return files\n    except Exception as e:\n        print(f\"Error listing files in {directory}: {e}\")\n        return []\n\n\ndef load_specific_datasets(data_dirs):\n    datasets = {}\n\n    # Million Song Dataset (Train & Test)\n    million_song_files = list_files(data_dirs[\"million_song\"])\n    if \"Testing_set_songs.csv\" in million_song_files and \"Training_set_songs.csv\" in million_song_files:\n        datasets[\"million_song\"] = {\n            \"train\": pd.read_csv(os.path.join(data_dirs[\"million_song\"], \"Training_set_songs.csv\")),\n            \"test\": pd.read_csv(os.path.join(data_dirs[\"million_song\"], \"Testing_set_songs.csv\"))\n        }\n    else:\n        print(\"Error: Test or Train files not found in Million Song Dataset.\")\n\n    # Million Song Dataset Studies\n    studies_files = list_files(data_dirs[\"million_song_studies\"])\n    studies_data = [pd.read_csv(os.path.join(data_dirs[\"million_song_studies\"], file)) \n                     for file in studies_files if file.endswith(\".csv\")]\n    datasets[\"million_song_studies\"] = pd.concat(studies_data, ignore_index=True) if studies_data else None\n\n    # Spotify Dataset\n    spotify_files = list_files(data_dirs[\"spotify_songs\"])\n    spotify_data = {file.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_dirs[\"spotify_songs\"], file))\n                     for file in spotify_files if file.endswith(\".csv\")}\n    datasets[\"spotify_songs\"] = spotify_data if spotify_data else None\n\n    # Spotify User Behavior Dataset (HTML)\n    spotify_html_data = extract_tables_from_html(os.path.join(data_dirs[\"spotify_user_behavior\"], \"__results__.html\"))\n    datasets[\"spotify_user_behavior\"] = spotify_html_data[0] if spotify_html_data else None\n\n    # Spotify Song Prediction Dataset\n    prediction_files = list_files(data_dirs[\"spotify_song_prediction\"])\n    prediction_data = [pd.read_csv(os.path.join(data_dirs[\"spotify_song_prediction\"], file)) \n                        for file in prediction_files if file.endswith(\".csv\")]\n    datasets[\"spotify_song_prediction\"] = pd.concat(prediction_data, ignore_index=True) if prediction_data else None\n\n    # Musics Demographic Data (HTML)\n    musics_html_data = extract_tables_from_html(os.path.join(data_dirs[\"musics_demographic_data\"], \"__results__.html\"))\n    datasets[\"musics_demographic_data\"] = musics_html_data[0] if musics_html_data else None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:10:40.454008Z","iopub.execute_input":"2025-02-19T04:10:40.454464Z","iopub.status.idle":"2025-02-19T04:10:41.011604Z","shell.execute_reply.started":"2025-02-19T04:10:40.454432Z","shell.execute_reply":"2025-02-19T04:10:41.01032Z"}},"outputs":[{"name":"stdout","text":"\nSpotify User Behavior Data Sample:\n  Unnamed: 0    Age  Gender spotify_usage_period  \\\n0          0  20-35  Female    More than 2 years   \n1          1  12-20    Male    More than 2 years   \n2          2  35-60  Others   6 months to 1 year   \n3          3  20-35  Female    1 year to 2 years   \n4          4  20-35  Female    1 year to 2 years   \n\n                         spotify_listening_device spotify_subscription_plan  \\\n0              Smart speakers or voice assistants       Free (ad-supported)   \n1                              Computer or laptop       Free (ad-supported)   \n2              Smart speakers or voice assistants       Free (ad-supported)   \n3  Smartphone, Smart speakers or voice assistants       Free (ad-supported)   \n4                                      Smartphone       Free (ad-supported)   \n\n  premium_sub_willingness          preffered_premium_plan  \\\n0                     Yes        Family Plan-Rs 179/month   \n1                     Yes  Individual Plan- Rs 119/ month   \n2                     Yes        Student Plan-Rs 59/month   \n3                      No                             NaN   \n4                      No                             NaN   \n\n  preferred_listening_content fav_music_genre  ...  \\\n0                     Podcast          Melody  ...   \n1                     Podcast             Rap  ...   \n2                     Podcast             Pop  ...   \n3                       Music          Melody  ...   \n4                       Music          Melody  ...   \n\n                              music_Influencial_mood  \\\n0                              Sadness or melancholy   \n1                       Social gatherings or parties   \n2                       Relaxation and stress relief   \n3  Relaxation and stress relief, Social gathering...   \n4                       Relaxation and stress relief   \n\n                           music_lis_frequency           music_expl_method  \\\n0                                 leisure time                   Playlists   \n1                              Workout session                   Playlists   \n2                 Study Hours, While Traveling                   Playlists   \n3  Office hours, Workout session, leisure time  recommendations, Playlists   \n4                                 leisure time  recommendations, Playlists   \n\n  music_recc_rating     pod_lis_frequency         fav_pod_genre  \\\n0                 3                 Daily                Comedy   \n1                 2  Several times a week                Comedy   \n2                 4           Once a week                Sports   \n3                 4                 Never                   NaN   \n4                 4                Rarely  Lifestyle and Health   \n\n  preffered_pod_format     pod_host_preference preffered_pod_duration  \\\n0            Interview                    Both                   Both   \n1            Interview                    Both                    NaN   \n2            Interview                     NaN                   Both   \n3                  NaN                     NaN                    NaN   \n4        Story telling  Well known individuals                   Both   \n\n  pod_variety_satisfaction  \n0                       Ok  \n1                Satisfied  \n2                Satisfied  \n3                       Ok  \n4                       Ok  \n\n[5 rows x 21 columns]\n\nMusics Demographic Data Sample:\n   Unnamed: 0  Music  Slow songs or fast songs  Dance  Folk  Country  \\\n0           0    5.0                       3.0    2.0   1.0      2.0   \n1           1    4.0                       4.0    2.0   1.0      1.0   \n\n   Classical music  Musical  Pop  Rock  ...   Age  Height  Weight  \\\n0              2.0      1.0  5.0   5.0  ...  20.0   163.0    48.0   \n1              1.0      2.0  3.0   5.0  ...  19.0   163.0    58.0   \n\n   Number of siblings  Gender  Left - right handed                Education  \\\n0                 1.0  female         right handed  college/bachelor degree   \n1                 2.0  female         right handed  college/bachelor degree   \n\n   Only child  Village - town  House - block of flats  \n0          no         village          block of flats  \n1          no            city          block of flats  \n\n[2 rows x 151 columns]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## [3] Load Data\n\nHere, we define a `data_dirs` dictionary for our dataset paths and use `load_specific_datasets` to create a unified dictionary `all_datasets`. Adjust the paths and file names to match your actual environment.\n","metadata":{}},{"cell_type":"code","source":"# =========================================\n# 4. Load Data\n# =========================================\ndata_dirs = {\n    \"million_song\": \"/kaggle/input/million-song-dataset\",\n    \"spotify_songs\": \"/kaggle/input/spotify-dataset\",\n    \"million_song_studies\": \"/kaggle/input/million-song-dataset-studies\",\n    \"spotify_user_behavior\": \"/kaggle/input/spotify-user-behavior-analysis\",\n    \"spotify_song_prediction\": \"/kaggle/input/spotify-song-prediction-and-recommendation-system\",\n    \"musics_demographic_data\": \"/kaggle/input/musics-depending-on-demographic-data\"\n}\n\nall_datasets = load_specific_datasets(data_dirs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:14:48.488839Z","iopub.execute_input":"2025-02-19T04:14:48.489234Z","iopub.status.idle":"2025-02-19T04:14:48.502225Z","shell.execute_reply.started":"2025-02-19T04:14:48.489208Z","shell.execute_reply":"2025-02-19T04:14:48.501169Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from: million_song -> /kaggle/input/million-song-dataset\nFiles in /kaggle/input/million-song-dataset: ['Testing_set_songs.csv', 'Training_set_songs.csv']\nLoading dataset from: spotify_songs -> /kaggle/input/spotify-dataset\nFiles in /kaggle/input/spotify-dataset: ['data']\nLoading dataset from: million_song_studies -> /kaggle/input/million-song-dataset-studies\nFiles in /kaggle/input/million-song-dataset-studies: ['song_data1.csv', 'song_data4.csv', 'song_data2.csv', 'song_data3.csv', 'song_data7.csv', 'song_data6.csv', 'song_data5.csv']\nLoading dataset from: spotify_user_behavior -> /kaggle/input/spotify-user-behavior-analysis\nFiles in /kaggle/input/spotify-user-behavior-analysis: ['__results__.html', '__notebook_source__.ipynb', '__resultx__.html', '__notebook__.ipynb', '__results___files', '__output__.json', 'custom.css']\nLoading dataset from: spotify_song_prediction -> /kaggle/input/spotify-song-prediction-and-recommendation-system\nFiles in /kaggle/input/spotify-song-prediction-and-recommendation-system: ['__results__.html', '__notebook_source__.ipynb', '__notebook__.ipynb', '__output__.json', 'custom.css']\nLoading dataset from: musics_demographic_data -> /kaggle/input/musics-depending-on-demographic-data\nFiles in /kaggle/input/musics-depending-on-demographic-data: ['__results__.html', '__results___files', '__output__.json', 'custom.css']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# =========================================\n# 5. Data Cleaning\n# =========================================\n# =========================================\n# 5. Data Cleaning\n# =========================================\ndef handle_missing_values(df):\n    \"\"\"\n    Handles missing values in a DataFrame by filling numerical columns with their mean \n    and categorical columns with their mode.\n    \n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n\n    Returns:\n    pandas.DataFrame: A copy of the DataFrame with missing values filled.\n    \"\"\"\n    if df is None:\n        print(\"Error: DataFrame is None.\")\n        return None\n\n    print(\"Missing values before handling:\")\n    print(df.isnull().sum())\n\n    # Create a copy of the DataFrame to avoid modifying the original object\n    df = df.copy()\n\n    # Fill numerical columns with mean and categorical columns with mode\n    for col in df.columns:\n        if df[col].dtype in ['float64', 'int64']:\n            df[col] = df[col].fillna(df[col].mean())\n        else:\n            # Using mode with error handling in case mode returns an empty Series\n            mode_val = df[col].mode()\n            if not mode_val.empty:\n                df[col] = df[col].fillna(mode_val[0])\n            else:\n                df[col] = df[col].fillna(\"Unknown\")\n\n    print(\"Missing values after handling:\")\n    print(df.isnull().sum())\n    return df\n\n# Load all datasets\nall_datasets = load_specific_datasets(data_dirs)\n\n# Handle missing values for Million Song Dataset\nmillion_song_data = all_datasets.get(\"million_song\")\nif million_song_data is not None:\n    million_song_data[\"train\"] = handle_missing_values(million_song_data[\"train\"])\n    million_song_data[\"test\"] = handle_missing_values(million_song_data[\"test\"])\n\n# Handle missing values for Spotify Dataset\nspotify_data = all_datasets.get(\"spotify_songs\")\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        spotify_data[key] = handle_missing_values(df)\n\n# Handle missing values for Million Song Studies\nmillion_song_studies = all_datasets.get(\"million_song_studies\")\nif million_song_studies is not None:\n    million_song_studies = handle_missing_values(million_song_studies)\n\n# Handle missing values for Spotify User Behavior Dataset\nspotify_user_behavior = all_datasets.get(\"spotify_user_behavior\")\nif spotify_user_behavior is not None:\n    spotify_user_behavior = handle_missing_values(spotify_user_behavior)\n\n# Handle missing values for Spotify Song Prediction Dataset\nspotify_song_prediction = all_datasets.get(\"spotify_song_prediction\")\nif spotify_song_prediction is not None:\n    spotify_song_prediction = handle_missing_values(spotify_song_prediction)\n\n# Handle missing values for Musics Demographic Data\nmusics_demographic_data = all_datasets.get(\"musics_demographic_data\")\nif musics_demographic_data is not None:\n    musics_demographic_data = handle_missing_values(musics_demographic_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:16:51.371257Z","iopub.execute_input":"2025-02-19T04:16:51.371614Z","iopub.status.idle":"2025-02-19T04:16:51.392606Z","shell.execute_reply.started":"2025-02-19T04:16:51.37159Z","shell.execute_reply":"2025-02-19T04:16:51.391443Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from: million_song -> /kaggle/input/million-song-dataset\nFiles in /kaggle/input/million-song-dataset: ['Testing_set_songs.csv', 'Training_set_songs.csv']\nLoading dataset from: spotify_songs -> /kaggle/input/spotify-dataset\nFiles in /kaggle/input/spotify-dataset: ['data']\nLoading dataset from: million_song_studies -> /kaggle/input/million-song-dataset-studies\nFiles in /kaggle/input/million-song-dataset-studies: ['song_data1.csv', 'song_data4.csv', 'song_data2.csv', 'song_data3.csv', 'song_data7.csv', 'song_data6.csv', 'song_data5.csv']\nLoading dataset from: spotify_user_behavior -> /kaggle/input/spotify-user-behavior-analysis\nFiles in /kaggle/input/spotify-user-behavior-analysis: ['__results__.html', '__notebook_source__.ipynb', '__resultx__.html', '__notebook__.ipynb', '__results___files', '__output__.json', 'custom.css']\nLoading dataset from: spotify_song_prediction -> /kaggle/input/spotify-song-prediction-and-recommendation-system\nFiles in /kaggle/input/spotify-song-prediction-and-recommendation-system: ['__results__.html', '__notebook_source__.ipynb', '__notebook__.ipynb', '__output__.json', 'custom.css']\nLoading dataset from: musics_demographic_data -> /kaggle/input/musics-depending-on-demographic-data\nFiles in /kaggle/input/musics-depending-on-demographic-data: ['__results__.html', '__results___files', '__output__.json', 'custom.css']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def normalize_features(df, columns=None):\n    if df is None:\n        print(\"Error: DataFrame is None.\")\n        return None\n\n    scaler = MinMaxScaler()\n    df = df.copy()\n\n    if columns is None:\n        columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n    available_columns = [col for col in columns if col in df.columns]\n    if not available_columns:\n        print(f\"No matching columns found for normalization. DataFrame columns: {df.columns.tolist()}\")\n        return df\n\n    df[available_columns] = scaler.fit_transform(df[available_columns])\n    return df\n\n# Load all datasets\nall_datasets = load_specific_datasets(data_dirs)\n\n# Normalize Million Song Dataset\nmillion_song_data = all_datasets.get(\"million_song\")\nif million_song_data is not None:\n    million_song_data[\"train\"] = normalize_features(million_song_data[\"train\"])\n    million_song_data[\"test\"] = normalize_features(million_song_data[\"test\"])\n\n# Normalize Spotify Dataset\nspotify_data = all_datasets.get(\"spotify_songs\")\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        spotify_data[key] = normalize_features(df)\n\n# Normalize Million Song Studies\nmillion_song_studies = all_datasets.get(\"million_song_studies\")\nif million_song_studies is not None:\n    million_song_studies = normalize_features(million_song_studies)\n\n# Normalize Spotify User Behavior Dataset\nspotify_user_behavior = all_datasets.get(\"spotify_user_behavior\")\nif spotify_user_behavior is not None:\n    spotify_user_behavior = normalize_features(spotify_user_behavior)\n\n# Normalize Spotify Song Prediction Dataset\nspotify_song_prediction = all_datasets.get(\"spotify_song_prediction\")\nif spotify_song_prediction is not None:\n    spotify_song_prediction = normalize_features(spotify_song_prediction)\n\n# Normalize Musics Demographic Data\nmusics_demographic_data = all_datasets.get(\"musics_demographic_data\")\nif musics_demographic_data is not None:\n    musics_demographic_data = normalize_features(musics_demographic_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:20:57.220565Z","iopub.execute_input":"2025-02-16T14:20:57.220877Z","iopub.status.idle":"2025-02-16T14:21:07.384558Z","shell.execute_reply.started":"2025-02-16T14:20:57.220831Z","shell.execute_reply":"2025-02-16T14:21:07.383681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_categorical_features(df):\n    if df is None:\n        print(\"Error: DataFrame is None.\")\n        return None, {}\n\n    df = df.copy()\n    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    print(f\"Detected categorical columns for encoding: {categorical_columns}\")\n\n    label_encoders = {}\n    for col in categorical_columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col].astype(str))\n        label_encoders[col] = le\n        print(f\"Encoded column: {col}\")\n\n    return df, label_encoders\n\n# Load all datasets\nall_datasets = load_specific_datasets(data_dirs)\n\n# Normalize Million Song Dataset\nmillion_song_data = all_datasets.get(\"million_song\")\nif million_song_data is not None:\n    million_song_data[\"train\"] = normalize_features(million_song_data[\"train\"])\n    million_song_data[\"test\"] = normalize_features(million_song_data[\"test\"])\n\n# Normalize Spotify Dataset\nspotify_data = all_datasets.get(\"spotify_songs\")\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        spotify_data[key] = normalize_features(df)\n\n# Normalize Million Song Studies\nmillion_song_studies = all_datasets.get(\"million_song_studies\")\nif million_song_studies is not None:\n    million_song_studies = normalize_features(million_song_studies)\n\n# Normalize Spotify User Behavior Dataset\nspotify_user_behavior = all_datasets.get(\"spotify_user_behavior\")\nif spotify_user_behavior is not None:\n    spotify_user_behavior = normalize_features(spotify_user_behavior)\n\n# Encode Categorical Features\nif million_song_data is not None:\n    print(\"Encoding Million Song Train DataFrame:\")\n    million_song_data[\"train\"], million_song_label_encoders = encode_categorical_features(million_song_data[\"train\"])\n    print(\"Encoding Million Song Test DataFrame:\")\n    million_song_data[\"test\"], _ = encode_categorical_features(million_song_data[\"test\"])\n\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        print(f\"Encoding Spotify DataFrame: {key}\")\n        spotify_data[key], spotify_label_encoders = encode_categorical_features(df)\n\nif million_song_studies is not None:\n    print(\"Encoding Million Song Studies DataFrame:\")\n    million_song_studies, million_song_studies_encoders = encode_categorical_features(million_song_studies)\n\nif spotify_user_behavior is not None:\n    print(\"Encoding Spotify User Behavior DataFrame:\")\n    spotify_user_behavior, spotify_user_behavior_encoders = encode_categorical_features(spotify_user_behavior)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:07.385646Z","iopub.execute_input":"2025-02-16T14:21:07.385855Z","iopub.status.idle":"2025-02-16T14:21:18.011567Z","shell.execute_reply.started":"2025-02-16T14:21:07.385838Z","shell.execute_reply":"2025-02-16T14:21:18.010567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *EDA Section*","metadata":{}},{"cell_type":"code","source":"# Suppress FutureWarnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Selected music-related features\nmusic_features = [\n    'duration', 'key', 'tempo', 'time_signature', 'end_of_fade_in',\n    'start_of_fade_out', 'loudness'\n]\n\n# Combine numerical data from multiple datasets\ndef combine_numerical_data(datasets):\n    combined_data = pd.DataFrame()\n    \n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n\n    return combined_data\n\n# Load and combine all datasets\nall_datasets = load_specific_datasets(data_dirs)\ncombined_data = combine_numerical_data(all_datasets)\n\n# Filter combined_data to include only the selected features\navailable_music_features = [feature for feature in music_features if feature in combined_data.columns]\nmusic_data = combined_data[available_music_features]\n\n# Convert non-numeric columns to numeric where possible using .loc to avoid SettingWithCopyWarning\nfor col in music_data.columns:\n    music_data.loc[:, col] = pd.to_numeric(music_data[col], errors='coerce')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:18.012844Z","iopub.execute_input":"2025-02-16T14:21:18.013197Z","iopub.status.idle":"2025-02-16T14:21:28.083138Z","shell.execute_reply.started":"2025-02-16T14:21:18.013162Z","shell.execute_reply":"2025-02-16T14:21:28.082455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic Statistics\nsummary_stats = music_data.describe()\nprint(\"Summary Statistics:\")\nprint(summary_stats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:29.709997Z","iopub.execute_input":"2025-02-16T14:21:29.71034Z","iopub.status.idle":"2025-02-16T14:21:29.809557Z","shell.execute_reply.started":"2025-02-16T14:21:29.710311Z","shell.execute_reply":"2025-02-16T14:21:29.808651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppress RuntimeWarnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n# Correlation Matrix\ncorrelation_matrix = music_data.corr()\n\n# Visualization: Correlation Heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Music Feature Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:31.561475Z","iopub.execute_input":"2025-02-16T14:21:31.561794Z","iopub.status.idle":"2025-02-16T14:21:31.960857Z","shell.execute_reply.started":"2025-02-16T14:21:31.561765Z","shell.execute_reply":"2025-02-16T14:21:31.960056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pairplot for key numerical features (select first 5 for readability)\nkey_features = music_data.select_dtypes(include=['float64', 'int64']).columns[:7]\nsns.pairplot(music_data[key_features])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:35.586488Z","iopub.execute_input":"2025-02-16T14:21:35.586776Z","iopub.status.idle":"2025-02-16T14:22:45.381188Z","shell.execute_reply.started":"2025-02-16T14:21:35.586754Z","shell.execute_reply":"2025-02-16T14:22:45.380148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of representative features\nfor feature in key_features:\n    plt.figure(figsize=(8, 4))\n    sns.histplot(music_data[feature], bins=30, kde=True)\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:22:45.382343Z","iopub.execute_input":"2025-02-16T14:22:45.382579Z","iopub.status.idle":"2025-02-16T14:22:53.720903Z","shell.execute_reply.started":"2025-02-16T14:22:45.382558Z","shell.execute_reply":"2025-02-16T14:22:53.719998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Kmeans clustering*","metadata":{}},{"cell_type":"markdown","source":"## [6] K-Means Clustering\n\nWe demonstrate **K-Means** on a subset of columns (e.g., `duration`, `tempo`, `start_of_fade_out`, etc.).  \n\n1. **Select features** for clustering.  \n2. **Scale** them with `StandardScaler`.  \n3. Use the **Elbow Method** to find a good number of clusters.  \n4. Fit K-Means and label the data with cluster IDs.  \n5. (Optional) **Label** each cluster with a descriptive name.  \n6. **Visualize** clusters via PCA.\n","metadata":{}},{"cell_type":"code","source":"music_features = [\n    'duration', 'key', 'tempo', 'time_signature', 'end_of_fade_in', 'start_of_fade_out'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:00.412702Z","iopub.execute_input":"2025-02-16T14:23:00.412995Z","iopub.status.idle":"2025-02-16T14:23:00.416744Z","shell.execute_reply.started":"2025-02-16T14:23:00.412974Z","shell.execute_reply":"2025-02-16T14:23:00.415931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine numerical data from multiple datasets\ndef combine_numerical_data(datasets):\n    combined_data = pd.DataFrame()\n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n    return combined_data\n\n# Load and combine datasets\nall_datasets = load_specific_datasets(data_dirs)\ncombined_data = combine_numerical_data(all_datasets)\n\n# 🎯 Step 3: Filter for Selected Music Features\navailable_music_features = [feature for feature in music_features if feature in combined_data.columns]\nmusic_data = combined_data[available_music_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:06.232081Z","iopub.execute_input":"2025-02-16T14:23:06.232383Z","iopub.status.idle":"2025-02-16T14:23:16.197909Z","shell.execute_reply.started":"2025-02-16T14:23:06.23236Z","shell.execute_reply":"2025-02-16T14:23:16.196979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned = music_data.copy()\n\nfor col in music_features:\n    if df_cleaned[col].dtype in ['float64', 'int64']:\n        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n    else:\n        # If there are any categorical features (unlikely here), fill with mode\n        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:25.265401Z","iopub.execute_input":"2025-02-16T14:23:25.265698Z","iopub.status.idle":"2025-02-16T14:23:25.328923Z","shell.execute_reply.started":"2025-02-16T14:23:25.265675Z","shell.execute_reply":"2025-02-16T14:23:25.328079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_cleaned[music_features])\n\n# Determine optimal number of clusters\nsse = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    preds = kmeans.fit_predict(scaled_features)\n    sse.append(kmeans.inertia_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:35.390325Z","iopub.execute_input":"2025-02-16T14:23:35.390608Z","iopub.status.idle":"2025-02-16T14:24:05.698252Z","shell.execute_reply.started":"2025-02-16T14:23:35.390585Z","shell.execute_reply":"2025-02-16T14:24:05.697389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot SSE for the Elbow Method\nplt.figure(figsize=(8, 4))\nplt.plot(k_range, sse, marker='o')\nplt.title('Elbow Method for Optimal Clusters')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Sum of Squared Errors (SSE)')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:24:05.699592Z","iopub.execute_input":"2025-02-16T14:24:05.699928Z","iopub.status.idle":"2025-02-16T14:24:05.865841Z","shell.execute_reply.started":"2025-02-16T14:24:05.699895Z","shell.execute_reply":"2025-02-16T14:24:05.864808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define Available Music Features\nmusic_features = [\n    'duration', 'key', 'tempo', 'time_signature', 'end_of_fade_in', 'start_of_fade_out'\n]\n\n# Step 2: Handle Missing Values by Filling with Mean\ndf_cleaned = music_data.copy()\nfor col in music_features:\n    if df_cleaned[col].isnull().sum() > 0:\n        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n\n# Step 3: Scale the Features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_cleaned[music_features])\n\n# Step 4: Apply K-Means Clustering\noptimal_k = 3\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cleaned['Cluster'] = kmeans.fit_predict(scaled_features)\n\n# Step 5: Inverse Scaling for Interpretation\ncentroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=music_features)\n\n# Step 6: Enhanced Labeling Based on Feature Dominance\n# Step 6: Enhanced Labeling Based on Feature Dominance\ndef label_cluster_by_centroid(centroid, centroids):\n    # Thresholds based on quartiles\n    tempo_threshold = centroids['tempo'].quantile(0.75)\n    fade_out_threshold = centroids['start_of_fade_out'].quantile(0.75)\n    duration_threshold = centroids['duration'].quantile(0.75)\n\n    # Labeling Logic for 3 Clusters\n    if centroid['start_of_fade_out'] >= fade_out_threshold:\n        return \"folks\"    # Cluster 1: Focus on long fade-out\n    elif centroid['tempo'] >= tempo_threshold:\n        return \"pop\"       # Cluster 2: Focus on fast tempo\n    else:\n        return \"jazz\"      # Cluster 3: Everything else, relaxed mood\n\n\n# Step 7: Assign Cluster Labels\ncluster_labels = {i: label_cluster_by_centroid(centroids.iloc[i], centroids) for i in range(optimal_k)}\ndf_cleaned['Cluster_Label'] = df_cleaned['Cluster'].map(cluster_labels)\n\n# Step 8: Display Final Results\nprint(\"Cluster Centroids:\")\nprint(centroids)\n\nprint(\"\\nSample Clustered Data:\")\nprint(df_cleaned[['duration', 'tempo', 'key', 'Cluster', 'Cluster_Label']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:24:09.129389Z","iopub.execute_input":"2025-02-16T14:24:09.129727Z","iopub.status.idle":"2025-02-16T14:24:11.13624Z","shell.execute_reply.started":"2025-02-16T14:24:09.129694Z","shell.execute_reply":"2025-02-16T14:24:11.135306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PCA for Visualization\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_features)\ndf_cleaned['PCA1'] = pca_result[:, 0]\ndf_cleaned['PCA2'] = pca_result[:, 1]\n\n# Map cluster labels\ncluster_label_map = df_cleaned.groupby(\"Cluster\")[\"Cluster_Label\"].first().to_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:26:14.319932Z","iopub.execute_input":"2025-02-16T14:26:14.320259Z","iopub.status.idle":"2025-02-16T14:26:14.853175Z","shell.execute_reply.started":"2025-02-16T14:26:14.320235Z","shell.execute_reply":"2025-02-16T14:26:14.852467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Clusters\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(df_cleaned['PCA1'], df_cleaned['PCA2'], c=df_cleaned['Cluster'], cmap='viridis', s=10)\n\n# Plot Cluster Centers\ncluster_centers = pca.transform(kmeans.cluster_centers_)\nplt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='X', s=100, label='Cluster Centers')\n\n# Label Cluster Centers\nfor i, (x, y) in enumerate(cluster_centers):\n    cluster_name = cluster_label_map.get(i, f\"Cluster {i}\")\n    plt.text(x, y, cluster_name, fontsize=12, fontweight='bold', ha='center', color='black',\n             bbox=dict(facecolor='white', alpha=0.6))\n\n# Final Touches\nplt.title('Clusters Visualization (PCA)')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.colorbar(scatter, label='Cluster')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:26:17.328823Z","iopub.execute_input":"2025-02-16T14:26:17.329184Z","iopub.status.idle":"2025-02-16T14:26:25.653363Z","shell.execute_reply.started":"2025-02-16T14:26:17.329155Z","shell.execute_reply":"2025-02-16T14:26:25.652384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(centroids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:27:35.77573Z","iopub.execute_input":"2025-02-16T14:27:35.776084Z","iopub.status.idle":"2025-02-16T14:27:35.783403Z","shell.execute_reply.started":"2025-02-16T14:27:35.776051Z","shell.execute_reply":"2025-02-16T14:27:35.782515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'df_cleaned' contains song features and 'kmeans.labels_' holds cluster labels\ndf_cleaned[\"mood_cluster\"] = kmeans.labels_\n\n# Save to CSV\ndf_cleaned.to_csv(\"clustered_songs.csv\", index=False)\nprint(\"Clustered dataset saved as clustered_songs.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:27:42.847463Z","iopub.execute_input":"2025-02-16T14:27:42.847764Z","iopub.status.idle":"2025-02-16T14:27:48.25415Z","shell.execute_reply.started":"2025-02-16T14:27:42.847738Z","shell.execute_reply":"2025-02-16T14:27:48.25322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Mood prediction model**","metadata":{}},{"cell_type":"code","source":"# ✅ Using the HTML-extracted DataFrame\ndf_cleaned = spotify_user_behavior.copy()\n\n# ✅ Encode Categorical Features\ncategorical_features = ['Gender', 'spotify_subscription_plan', 'preferred_listening_content', \n                         'fav_music_genre', 'music_time_slot', 'music_expl_method']\n\nlabel_encoders = {}\nfor col in categorical_features:\n    le = LabelEncoder()\n    df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n    label_encoders[col] = le\n\n# ✅ Encode the Target Variable (User Mood)\nlabel_encoder_mood = LabelEncoder()\ndf_cleaned['Encoded_Mood'] = label_encoder_mood.fit_transform(df_cleaned['music_Influencial_mood'].astype(str))\n\n# ✅ Select Features and Target\nfeatures = ['Gender', 'spotify_subscription_plan', 'preferred_listening_content', \n            'fav_music_genre', 'music_time_slot', 'music_recc_rating', 'music_expl_method']\nX = df_cleaned[features]\ny = df_cleaned['Encoded_Mood']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:27:54.225313Z","iopub.execute_input":"2025-02-16T14:27:54.225639Z","iopub.status.idle":"2025-02-16T14:27:54.237218Z","shell.execute_reply.started":"2025-02-16T14:27:54.225609Z","shell.execute_reply":"2025-02-16T14:27:54.236296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Scaling\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ✅ Polynomial Features\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n\n# ✅ Resampling with RandomOverSampler\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X_poly, y)\n\n# ✅ Train-test Split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:04.277255Z","iopub.execute_input":"2025-02-16T14:28:04.277538Z","iopub.status.idle":"2025-02-16T14:28:04.289834Z","shell.execute_reply.started":"2025-02-16T14:28:04.277516Z","shell.execute_reply":"2025-02-16T14:28:04.289011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Logistic Regression Model with Regularization\nclass UserMoodPredictionModel:\n    def __init__(self, learning_rate=0.005, n_iterations=30000, regularization_strength=0.05):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.reg_strength = regularization_strength\n        self.weights = None\n        self.bias = None\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.classes = np.unique(y)\n        self.weights = np.zeros((len(self.classes), n_features))\n        self.bias = np.zeros(len(self.classes))\n\n        for idx, cls in enumerate(self.classes):\n            y_binary = np.where(y == cls, 1, 0)\n            for _ in range(self.n_iterations):\n                linear_model = np.dot(X, self.weights[idx]) + self.bias[idx]\n                y_predicted = self.sigmoid(linear_model)\n\n                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y_binary)) + self.reg_strength * self.weights[idx]\n                db = (1 / n_samples) * np.sum(y_predicted - y_binary)\n\n                self.weights[idx] -= self.learning_rate * dw\n                self.bias[idx] -= self.learning_rate * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights.T) + self.bias\n        y_predicted = self.sigmoid(linear_model)\n        return np.argmax(y_predicted, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:06.356793Z","iopub.execute_input":"2025-02-16T14:28:06.357167Z","iopub.status.idle":"2025-02-16T14:28:06.367075Z","shell.execute_reply.started":"2025-02-16T14:28:06.357124Z","shell.execute_reply":"2025-02-16T14:28:06.366126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Initialize and Train the Logistic Regression Model\nlogistic_model = UserMoodPredictionModel()\nlogistic_model.fit(X_train, y_train)\n\n# ✅ Random Forest Classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# ✅ Predictions\nlogistic_pred = logistic_model.predict(X_test)\nrf_pred = rf_model.predict(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:08.397365Z","iopub.execute_input":"2025-02-16T14:28:08.397763Z","iopub.status.idle":"2025-02-16T14:28:14.796311Z","shell.execute_reply.started":"2025-02-16T14:28:08.39773Z","shell.execute_reply":"2025-02-16T14:28:14.795386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Evaluation Function\ndef evaluate_model(y_true, y_pred, model_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n\n    print(f\"\\n{model_name} Performance:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n\n    unique_classes = np.unique(y_true)\n    target_names = label_encoder_mood.inverse_transform(unique_classes)\n\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, labels=unique_classes, target_names=target_names, zero_division=1))\n\n    # ✅ Confusion Matrix\n    conf_mat = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.xlabel('Predicted Mood')\n    plt.ylabel('Actual Mood')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:37.264485Z","iopub.execute_input":"2025-02-16T14:28:37.264805Z","iopub.status.idle":"2025-02-16T14:28:37.27107Z","shell.execute_reply.started":"2025-02-16T14:28:37.26478Z","shell.execute_reply":"2025-02-16T14:28:37.270121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Model Evaluation\nevaluate_model(y_test, logistic_pred, \"Logistic Regression\")\nevaluate_model(y_test, rf_pred, \"Random Forest Classifier\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:41.226495Z","iopub.execute_input":"2025-02-16T14:28:41.226788Z","iopub.status.idle":"2025-02-16T14:28:41.707893Z","shell.execute_reply.started":"2025-02-16T14:28:41.226766Z","shell.execute_reply":"2025-02-16T14:28:41.707067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Recommendation System Neural Network**","metadata":{}},{"cell_type":"markdown","source":"\n* The dataset is loaded and features (such as valence, danceability, etc.) and target (mood_cluster) are selected.\n* Features are normalized using StandardScaler to improve model performance.\n* The dataset is split into training and testing sets using train_test_split.\n","metadata":{}},{"cell_type":"code","source":"selected_features = [\n    'valence', 'danceability', 'energy', 'acousticness', 'tempo'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:03.815854Z","iopub.execute_input":"2025-02-16T14:29:03.8162Z","iopub.status.idle":"2025-02-16T14:29:03.820085Z","shell.execute_reply.started":"2025-02-16T14:29:03.816171Z","shell.execute_reply":"2025-02-16T14:29:03.819056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine numerical data from multiple datasets\ndef combine_numerical_data(datasets):\n    combined_data = pd.DataFrame()\n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n    return combined_data\n\n# Load and combine datasets\nall_datasets = load_specific_datasets(data_dirs)\ncombined_data = combine_numerical_data(all_datasets)\n\n# 🎯 Step 3: Filter for Selected Music Features\navailable_music_features = [feature for feature in selected_features if feature in combined_data.columns]\nselected_music_data = combined_data[available_music_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:04.669682Z","iopub.execute_input":"2025-02-16T14:29:04.669968Z","iopub.status.idle":"2025-02-16T14:29:14.599558Z","shell.execute_reply.started":"2025-02-16T14:29:04.669947Z","shell.execute_reply":"2025-02-16T14:29:14.598479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the clustered dataset (assuming it contains song features and mood clusters)\ndata = pd.read_csv(\"clustered_songs.csv\")  # Update with actual file path\n\n# Select features and target\nfeatures = available_music_features\ntarget = 'mood_cluster'  # Mood cluster as label\n\nX = data[features]\ny = data[target]\n\n# Normalize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:18.017062Z","iopub.execute_input":"2025-02-16T14:29:18.017387Z","iopub.status.idle":"2025-02-16T14:29:18.839072Z","shell.execute_reply.started":"2025-02-16T14:29:18.017362Z","shell.execute_reply":"2025-02-16T14:29:18.838343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* A Sequential model is used to define the neural network.\n* It has two hidden layers with ReLU activation and dropout for regularization.\n* The output layer uses a softmax activation for multi-class classification, with the number of units equal to the number of unique mood clusters\n.","metadata":{}},{"cell_type":"code","source":"# Define the neural network model\nmodel = Sequential([\n    Input(shape=(X_train.shape[1],)),  # Define input layer explicitly\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(len(y.unique()), activation='softmax')  # Output layer for classification\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:22.160556Z","iopub.execute_input":"2025-02-16T14:29:22.160856Z","iopub.status.idle":"2025-02-16T14:29:24.226249Z","shell.execute_reply.started":"2025-02-16T14:29:22.160831Z","shell.execute_reply":"2025-02-16T14:29:24.225521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* The model is compiled with Adam optimizer and sparse_categorical_crossentropy loss function.\n* The model is trained for 50 epochs with a batch size of 16, using validation data to monitor performance.\n","metadata":{}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:33.757666Z","iopub.execute_input":"2025-02-16T14:29:33.757957Z","iopub.status.idle":"2025-02-16T15:06:21.164232Z","shell.execute_reply.started":"2025-02-16T14:29:33.757935Z","shell.execute_reply":"2025-02-16T15:06:21.163236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Model Accuracy Over Epochs')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Model Loss Over Epochs')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:11:29.633912Z","iopub.execute_input":"2025-02-16T15:11:29.634257Z","iopub.status.idle":"2025-02-16T15:11:29.990861Z","shell.execute_reply.started":"2025-02-16T15:11:29.634229Z","shell.execute_reply":"2025-02-16T15:11:29.989931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Save the trained model\nmodel.save(\"music_mood_recommendation_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:11:37.389972Z","iopub.execute_input":"2025-02-16T15:11:37.390286Z","iopub.status.idle":"2025-02-16T15:11:41.863721Z","shell.execute_reply.started":"2025-02-16T15:11:37.390263Z","shell.execute_reply":"2025-02-16T15:11:41.862911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
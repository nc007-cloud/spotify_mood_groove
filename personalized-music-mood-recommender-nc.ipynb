{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10049,"sourceType":"datasetVersion","datasetId":6985},{"sourceId":2936818,"sourceType":"datasetVersion","datasetId":1800580},{"sourceId":3995285,"sourceType":"datasetVersion","datasetId":2370674},{"sourceId":3949876,"sourceType":"kernelVersion"},{"sourceId":54619392,"sourceType":"kernelVersion"},{"sourceId":135900347,"sourceType":"kernelVersion"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Building Mood Based Music Recommendation System using Spotify dataset and the Million Song Dataset\nimage.png","metadata":{"execution":{"iopub.status.busy":"2025-02-16T05:14:51.550232Z","iopub.execute_input":"2025-02-16T05:14:51.550640Z","iopub.status.idle":"2025-02-16T05:14:51.558598Z","shell.execute_reply.started":"2025-02-16T05:14:51.550605Z","shell.execute_reply":"2025-02-16T05:14:51.556441Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nimport json\nimport nbformat\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom bs4 import BeautifulSoup\nfrom io import StringIO\nfrom imblearn.over_sampling import RandomOverSampler\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, RobustScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score\nfrom tensorflow.keras.models import Sequential\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:19:15.223483Z","iopub.execute_input":"2025-02-16T14:19:15.223768Z","iopub.status.idle":"2025-02-16T14:19:15.229484Z","shell.execute_reply.started":"2025-02-16T14:19:15.223745Z","shell.execute_reply":"2025-02-16T14:19:15.228435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:19:17.230291Z","iopub.execute_input":"2025-02-16T14:19:17.230567Z","iopub.status.idle":"2025-02-16T14:19:17.248252Z","shell.execute_reply.started":"2025-02-16T14:19:17.230544Z","shell.execute_reply":"2025-02-16T14:19:17.247451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dirs = {\n    \"million_song\" : \"/kaggle/input/million-song-dataset\",\n    \"spotify_songs\": \"/kaggle/input/spotify-dataset\",\n    \"million_song_studies\": \"/kaggle/input/million-song-dataset-studies\",\n    \"spotify_user_behavior\": \"/kaggle/input/spotify-user-behavior-analysis\",  # Integrated the Spotify User Behavior Analysis\n    \"spotify_song_prediction\": \"/kaggle/input/spotify-song-prediction-and-recommendation-system\",\n    \"musics_demographic_data\": \"/kaggle/input/musics-depending-on-demographic-data\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:19:35.261234Z","iopub.execute_input":"2025-02-16T14:19:35.261583Z","iopub.status.idle":"2025-02-16T14:19:35.265536Z","shell.execute_reply.started":"2025-02-16T14:19:35.261550Z","shell.execute_reply":"2025-02-16T14:19:35.264535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to extract tables from HTML files\ndef extract_tables_from_html(html_path):\n    try:\n        with open(html_path, 'r', encoding='utf-8') as file:\n            soup = BeautifulSoup(file, 'html.parser')\n\n        # Extract all tables from the HTML using StringIO to avoid FutureWarning\n        tables = pd.read_html(StringIO(str(soup)))\n        return tables\n    except Exception as e:\n        print(f\"Error reading HTML file: {e}\")\n        return None\n\n# Directories for datasets\ndata_dirs = {\n    \"spotify_user_behavior\": \"/kaggle/input/spotify-user-behavior-analysis\",\n    \"spotify_song_prediction\": \"/kaggle/input/spotify-song-prediction-and-recommendation-system\",\n    \"musics_demographic_data\": \"/kaggle/input/musics-depending-on-demographic-data\"\n}\n\n# Attempting to extract data from HTML files\nspotify_html_data = extract_tables_from_html(os.path.join(data_dirs[\"spotify_user_behavior\"], \"__results__.html\"))\nmusics_html_data = extract_tables_from_html(os.path.join(data_dirs[\"musics_demographic_data\"], \"__results__.html\"))\n\n# Display the first few rows of the extracted tables if available\nif spotify_html_data:\n    print(\"\\nSpotify User Behavior Data Sample:\")\n    print(spotify_html_data[0].head())  # Display the first table\nelse:\n    print(\"No data found in Spotify User Behavior HTML.\")\n\nif musics_html_data:\n    print(\"\\nMusics Demographic Data Sample:\")\n    print(musics_html_data[0].head())  # Display the first table\nelse:\n    print(\"No data found in Musics Demographic HTML.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:19:43.481115Z","iopub.execute_input":"2025-02-16T14:19:43.481406Z","iopub.status.idle":"2025-02-16T14:19:44.195704Z","shell.execute_reply.started":"2025-02-16T14:19:43.481383Z","shell.execute_reply":"2025-02-16T14:19:44.194846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_files(directory):\n    try:\n        files = os.listdir(directory)\n        print(f\"Files in {directory}:\", files)\n        return files\n    except Exception as e:\n        print(f\"Error listing files in {directory}: {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:20:27.886126Z","iopub.execute_input":"2025-02-16T14:20:27.886459Z","iopub.status.idle":"2025-02-16T14:20:27.890559Z","shell.execute_reply.started":"2025-02-16T14:20:27.886429Z","shell.execute_reply":"2025-02-16T14:20:27.889689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_specific_datasets(data_dirs):\n    datasets = {}\n\n    # Million Song Dataset (Train & Test)\n    million_song_files = list_files(data_dirs[\"million_song\"])\n    if \"Testing_set_songs.csv\" in million_song_files and \"Training_set_songs.csv\" in million_song_files:\n        datasets[\"million_song\"] = {\n            \"train\": pd.read_csv(os.path.join(data_dirs[\"million_song\"], \"Training_set_songs.csv\")),\n            \"test\": pd.read_csv(os.path.join(data_dirs[\"million_song\"], \"Testing_set_songs.csv\"))\n        }\n    else:\n        print(\"Error: Test or Train files not found in Million Song Dataset.\")\n\n    # Million Song Dataset Studies\n    studies_files = list_files(data_dirs[\"million_song_studies\"])\n    studies_data = [pd.read_csv(os.path.join(data_dirs[\"million_song_studies\"], file)) \n                     for file in studies_files if file.endswith(\".csv\")]\n    datasets[\"million_song_studies\"] = pd.concat(studies_data, ignore_index=True) if studies_data else None\n\n    # Spotify Dataset\n    spotify_files = list_files(data_dirs[\"spotify_songs\"])\n    spotify_data = {file.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_dirs[\"spotify_songs\"], file))\n                     for file in spotify_files if file.endswith(\".csv\")}\n    datasets[\"spotify_songs\"] = spotify_data if spotify_data else None\n\n    # Spotify User Behavior Dataset (HTML)\n    spotify_html_data = extract_tables_from_html(os.path.join(data_dirs[\"spotify_user_behavior\"], \"__results__.html\"))\n    datasets[\"spotify_user_behavior\"] = spotify_html_data[0] if spotify_html_data else None\n\n    # Spotify Song Prediction Dataset\n    prediction_files = list_files(data_dirs[\"spotify_song_prediction\"])\n    prediction_data = [pd.read_csv(os.path.join(data_dirs[\"spotify_song_prediction\"], file)) \n                        for file in prediction_files if file.endswith(\".csv\")]\n    datasets[\"spotify_song_prediction\"] = pd.concat(prediction_data, ignore_index=True) if prediction_data else None\n\n    # Musics Demographic Data (HTML)\n    musics_html_data = extract_tables_from_html(os.path.join(data_dirs[\"musics_demographic_data\"], \"__results__.html\"))\n    datasets[\"musics_demographic_data\"] = musics_html_data[0] if musics_html_data else None\n\n    return datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:20:35.372708Z","iopub.execute_input":"2025-02-16T14:20:35.373060Z","iopub.status.idle":"2025-02-16T14:20:35.381483Z","shell.execute_reply.started":"2025-02-16T14:20:35.373012Z","shell.execute_reply":"2025-02-16T14:20:35.380549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dirs = {\n    \"million_song\" : \"/kaggle/input/million-song-dataset\",\n    \"spotify_songs\": \"/kaggle/input/spotify-dataset\",\n    \"million_song_studies\": \"/kaggle/input/million-song-dataset-studies\",\n    \"spotify_user_behavior\": \"/kaggle/input/spotify-user-behavior-analysis\",  # Integrated the Spotify User Behavior Analysis\n    \"spotify_song_prediction\": \"/kaggle/input/spotify-song-prediction-and-recommendation-system\",\n    \"musics_demographic_data\": \"/kaggle/input/musics-depending-on-demographic-data\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:20:39.766936Z","iopub.execute_input":"2025-02-16T14:20:39.767280Z","iopub.status.idle":"2025-02-16T14:20:39.771127Z","shell.execute_reply.started":"2025-02-16T14:20:39.767256Z","shell.execute_reply":"2025-02-16T14:20:39.770117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def handle_missing_values(df):\n    if df is None:\n        print(\"Error: DataFrame is None.\")\n        return None\n\n    print(\"Missing values before handling:\")\n    print(df.isnull().sum())\n\n    # Create a copy of the DataFrame to avoid modifying the original object\n    df = df.copy()\n\n    # Fill numerical columns with mean and categorical columns with mode\n    for col in df.columns:\n        if df[col].dtype in ['float64', 'int64']:\n            df[col] = df[col].fillna(df[col].mean())\n        else:\n            df[col] = df[col].fillna(df[col].mode()[0])\n\n    print(\"Missing values after handling:\")\n    print(df.isnull().sum())\n    return df\n\n# Load all datasets\nall_datasets = load_specific_datasets(data_dirs)\n\n# Handle missing values for Million Song Dataset\nmillion_song_data = all_datasets.get(\"million_song\")\nif million_song_data is not None:\n    million_song_data[\"train\"] = handle_missing_values(million_song_data[\"train\"])\n    million_song_data[\"test\"] = handle_missing_values(million_song_data[\"test\"])\n\n# Handle missing values for Spotify Dataset\nspotify_data = all_datasets.get(\"spotify_songs\")\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        spotify_data[key] = handle_missing_values(df)\n\n# Handle missing values for Million Song Studies\nmillion_song_studies = all_datasets.get(\"million_song_studies\")\nif million_song_studies is not None:\n    million_song_studies = handle_missing_values(million_song_studies)\n\n# Handle missing values for Spotify User Behavior Dataset\nspotify_user_behavior = all_datasets.get(\"spotify_user_behavior\")\nif spotify_user_behavior is not None:\n    spotify_user_behavior = handle_missing_values(spotify_user_behavior)\n\n# Handle missing values for Spotify Song Prediction Dataset\nspotify_song_prediction = all_datasets.get(\"spotify_song_prediction\")\nif spotify_song_prediction is not None:\n    spotify_song_prediction = handle_missing_values(spotify_song_prediction)\n\n# Handle missing values for Musics Demographic Data\nmusics_demographic_data = all_datasets.get(\"musics_demographic_data\")\nif musics_demographic_data is not None:\n    musics_demographic_data = handle_missing_values(musics_demographic_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:20:42.288784Z","iopub.execute_input":"2025-02-16T14:20:42.289099Z","iopub.status.idle":"2025-02-16T14:20:57.219385Z","shell.execute_reply.started":"2025-02-16T14:20:42.289072Z","shell.execute_reply":"2025-02-16T14:20:57.218425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_features(df, columns=None):\n    if df is None:\n        print(\"Error: DataFrame is None.\")\n        return None\n\n    scaler = MinMaxScaler()\n    df = df.copy()\n\n    if columns is None:\n        columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n    available_columns = [col for col in columns if col in df.columns]\n    if not available_columns:\n        print(f\"No matching columns found for normalization. DataFrame columns: {df.columns.tolist()}\")\n        return df\n\n    df[available_columns] = scaler.fit_transform(df[available_columns])\n    return df\n\n# Load all datasets\nall_datasets = load_specific_datasets(data_dirs)\n\n# Normalize Million Song Dataset\nmillion_song_data = all_datasets.get(\"million_song\")\nif million_song_data is not None:\n    million_song_data[\"train\"] = normalize_features(million_song_data[\"train\"])\n    million_song_data[\"test\"] = normalize_features(million_song_data[\"test\"])\n\n# Normalize Spotify Dataset\nspotify_data = all_datasets.get(\"spotify_songs\")\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        spotify_data[key] = normalize_features(df)\n\n# Normalize Million Song Studies\nmillion_song_studies = all_datasets.get(\"million_song_studies\")\nif million_song_studies is not None:\n    million_song_studies = normalize_features(million_song_studies)\n\n# Normalize Spotify User Behavior Dataset\nspotify_user_behavior = all_datasets.get(\"spotify_user_behavior\")\nif spotify_user_behavior is not None:\n    spotify_user_behavior = normalize_features(spotify_user_behavior)\n\n# Normalize Spotify Song Prediction Dataset\nspotify_song_prediction = all_datasets.get(\"spotify_song_prediction\")\nif spotify_song_prediction is not None:\n    spotify_song_prediction = normalize_features(spotify_song_prediction)\n\n# Normalize Musics Demographic Data\nmusics_demographic_data = all_datasets.get(\"musics_demographic_data\")\nif musics_demographic_data is not None:\n    musics_demographic_data = normalize_features(musics_demographic_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:20:57.220565Z","iopub.execute_input":"2025-02-16T14:20:57.220877Z","iopub.status.idle":"2025-02-16T14:21:07.384558Z","shell.execute_reply.started":"2025-02-16T14:20:57.220831Z","shell.execute_reply":"2025-02-16T14:21:07.383681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_categorical_features(df):\n    if df is None:\n        print(\"Error: DataFrame is None.\")\n        return None, {}\n\n    df = df.copy()\n    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    print(f\"Detected categorical columns for encoding: {categorical_columns}\")\n\n    label_encoders = {}\n    for col in categorical_columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col].astype(str))\n        label_encoders[col] = le\n        print(f\"Encoded column: {col}\")\n\n    return df, label_encoders\n\n# Load all datasets\nall_datasets = load_specific_datasets(data_dirs)\n\n# Normalize Million Song Dataset\nmillion_song_data = all_datasets.get(\"million_song\")\nif million_song_data is not None:\n    million_song_data[\"train\"] = normalize_features(million_song_data[\"train\"])\n    million_song_data[\"test\"] = normalize_features(million_song_data[\"test\"])\n\n# Normalize Spotify Dataset\nspotify_data = all_datasets.get(\"spotify_songs\")\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        spotify_data[key] = normalize_features(df)\n\n# Normalize Million Song Studies\nmillion_song_studies = all_datasets.get(\"million_song_studies\")\nif million_song_studies is not None:\n    million_song_studies = normalize_features(million_song_studies)\n\n# Normalize Spotify User Behavior Dataset\nspotify_user_behavior = all_datasets.get(\"spotify_user_behavior\")\nif spotify_user_behavior is not None:\n    spotify_user_behavior = normalize_features(spotify_user_behavior)\n\n# Encode Categorical Features\nif million_song_data is not None:\n    print(\"Encoding Million Song Train DataFrame:\")\n    million_song_data[\"train\"], million_song_label_encoders = encode_categorical_features(million_song_data[\"train\"])\n    print(\"Encoding Million Song Test DataFrame:\")\n    million_song_data[\"test\"], _ = encode_categorical_features(million_song_data[\"test\"])\n\nif spotify_data is not None:\n    for key, df in spotify_data.items():\n        print(f\"Encoding Spotify DataFrame: {key}\")\n        spotify_data[key], spotify_label_encoders = encode_categorical_features(df)\n\nif million_song_studies is not None:\n    print(\"Encoding Million Song Studies DataFrame:\")\n    million_song_studies, million_song_studies_encoders = encode_categorical_features(million_song_studies)\n\nif spotify_user_behavior is not None:\n    print(\"Encoding Spotify User Behavior DataFrame:\")\n    spotify_user_behavior, spotify_user_behavior_encoders = encode_categorical_features(spotify_user_behavior)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:07.385646Z","iopub.execute_input":"2025-02-16T14:21:07.385855Z","iopub.status.idle":"2025-02-16T14:21:18.011567Z","shell.execute_reply.started":"2025-02-16T14:21:07.385838Z","shell.execute_reply":"2025-02-16T14:21:18.010567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *EDA Section*","metadata":{}},{"cell_type":"code","source":"# Suppress FutureWarnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Selected music-related features\nmusic_features = [\n    'duration', 'key', 'tempo', 'time_signature', 'end_of_fade_in',\n    'start_of_fade_out', 'loudness'\n]\n\n# Combine numerical data from multiple datasets\ndef combine_numerical_data(datasets):\n    combined_data = pd.DataFrame()\n    \n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n\n    return combined_data\n\n# Load and combine all datasets\nall_datasets = load_specific_datasets(data_dirs)\ncombined_data = combine_numerical_data(all_datasets)\n\n# Filter combined_data to include only the selected features\navailable_music_features = [feature for feature in music_features if feature in combined_data.columns]\nmusic_data = combined_data[available_music_features]\n\n# Convert non-numeric columns to numeric where possible using .loc to avoid SettingWithCopyWarning\nfor col in music_data.columns:\n    music_data.loc[:, col] = pd.to_numeric(music_data[col], errors='coerce')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:18.012844Z","iopub.execute_input":"2025-02-16T14:21:18.013197Z","iopub.status.idle":"2025-02-16T14:21:28.083138Z","shell.execute_reply.started":"2025-02-16T14:21:18.013162Z","shell.execute_reply":"2025-02-16T14:21:28.082455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic Statistics\nsummary_stats = music_data.describe()\nprint(\"Summary Statistics:\")\nprint(summary_stats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:29.709997Z","iopub.execute_input":"2025-02-16T14:21:29.710340Z","iopub.status.idle":"2025-02-16T14:21:29.809557Z","shell.execute_reply.started":"2025-02-16T14:21:29.710311Z","shell.execute_reply":"2025-02-16T14:21:29.808651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppress RuntimeWarnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n# Correlation Matrix\ncorrelation_matrix = music_data.corr()\n\n# Visualization: Correlation Heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Music Feature Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:31.561475Z","iopub.execute_input":"2025-02-16T14:21:31.561794Z","iopub.status.idle":"2025-02-16T14:21:31.960857Z","shell.execute_reply.started":"2025-02-16T14:21:31.561765Z","shell.execute_reply":"2025-02-16T14:21:31.960056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pairplot for key numerical features (select first 5 for readability)\nkey_features = music_data.select_dtypes(include=['float64', 'int64']).columns[:7]\nsns.pairplot(music_data[key_features])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:21:35.586488Z","iopub.execute_input":"2025-02-16T14:21:35.586776Z","iopub.status.idle":"2025-02-16T14:22:45.381188Z","shell.execute_reply.started":"2025-02-16T14:21:35.586754Z","shell.execute_reply":"2025-02-16T14:22:45.380148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of representative features\nfor feature in key_features:\n    plt.figure(figsize=(8, 4))\n    sns.histplot(music_data[feature], bins=30, kde=True)\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:22:45.382343Z","iopub.execute_input":"2025-02-16T14:22:45.382579Z","iopub.status.idle":"2025-02-16T14:22:53.720903Z","shell.execute_reply.started":"2025-02-16T14:22:45.382558Z","shell.execute_reply":"2025-02-16T14:22:53.719998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Kmeans clustering*","metadata":{}},{"cell_type":"code","source":"music_features = [\n    'duration', 'key', 'tempo', 'time_signature', 'end_of_fade_in', 'start_of_fade_out'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:00.412702Z","iopub.execute_input":"2025-02-16T14:23:00.412995Z","iopub.status.idle":"2025-02-16T14:23:00.416744Z","shell.execute_reply.started":"2025-02-16T14:23:00.412974Z","shell.execute_reply":"2025-02-16T14:23:00.415931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine numerical data from multiple datasets\ndef combine_numerical_data(datasets):\n    combined_data = pd.DataFrame()\n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n    return combined_data\n\n# Load and combine datasets\nall_datasets = load_specific_datasets(data_dirs)\ncombined_data = combine_numerical_data(all_datasets)\n\n# ðŸŽ¯ Step 3: Filter for Selected Music Features\navailable_music_features = [feature for feature in music_features if feature in combined_data.columns]\nmusic_data = combined_data[available_music_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:06.232081Z","iopub.execute_input":"2025-02-16T14:23:06.232383Z","iopub.status.idle":"2025-02-16T14:23:16.197909Z","shell.execute_reply.started":"2025-02-16T14:23:06.232360Z","shell.execute_reply":"2025-02-16T14:23:16.196979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned = music_data.copy()\n\nfor col in music_features:\n    if df_cleaned[col].dtype in ['float64', 'int64']:\n        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n    else:\n        # If there are any categorical features (unlikely here), fill with mode\n        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:25.265401Z","iopub.execute_input":"2025-02-16T14:23:25.265698Z","iopub.status.idle":"2025-02-16T14:23:25.328923Z","shell.execute_reply.started":"2025-02-16T14:23:25.265675Z","shell.execute_reply":"2025-02-16T14:23:25.328079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_cleaned[music_features])\n\n# Determine optimal number of clusters\nsse = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    preds = kmeans.fit_predict(scaled_features)\n    sse.append(kmeans.inertia_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:23:35.390325Z","iopub.execute_input":"2025-02-16T14:23:35.390608Z","iopub.status.idle":"2025-02-16T14:24:05.698252Z","shell.execute_reply.started":"2025-02-16T14:23:35.390585Z","shell.execute_reply":"2025-02-16T14:24:05.697389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot SSE for the Elbow Method\nplt.figure(figsize=(8, 4))\nplt.plot(k_range, sse, marker='o')\nplt.title('Elbow Method for Optimal Clusters')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Sum of Squared Errors (SSE)')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:24:05.699592Z","iopub.execute_input":"2025-02-16T14:24:05.699928Z","iopub.status.idle":"2025-02-16T14:24:05.865841Z","shell.execute_reply.started":"2025-02-16T14:24:05.699895Z","shell.execute_reply":"2025-02-16T14:24:05.864808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define Available Music Features\nmusic_features = [\n    'duration', 'key', 'tempo', 'time_signature', 'end_of_fade_in', 'start_of_fade_out'\n]\n\n# Step 2: Handle Missing Values by Filling with Mean\ndf_cleaned = music_data.copy()\nfor col in music_features:\n    if df_cleaned[col].isnull().sum() > 0:\n        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n\n# Step 3: Scale the Features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_cleaned[music_features])\n\n# Step 4: Apply K-Means Clustering\noptimal_k = 3\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cleaned['Cluster'] = kmeans.fit_predict(scaled_features)\n\n# Step 5: Inverse Scaling for Interpretation\ncentroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=music_features)\n\n# Step 6: Enhanced Labeling Based on Feature Dominance\n# Step 6: Enhanced Labeling Based on Feature Dominance\ndef label_cluster_by_centroid(centroid, centroids):\n    # Thresholds based on quartiles\n    tempo_threshold = centroids['tempo'].quantile(0.75)\n    fade_out_threshold = centroids['start_of_fade_out'].quantile(0.75)\n    duration_threshold = centroids['duration'].quantile(0.75)\n\n    # Labeling Logic for 3 Clusters\n    if centroid['start_of_fade_out'] >= fade_out_threshold:\n        return \"folks\"    # Cluster 1: Focus on long fade-out\n    elif centroid['tempo'] >= tempo_threshold:\n        return \"pop\"       # Cluster 2: Focus on fast tempo\n    else:\n        return \"jazz\"      # Cluster 3: Everything else, relaxed mood\n\n\n# Step 7: Assign Cluster Labels\ncluster_labels = {i: label_cluster_by_centroid(centroids.iloc[i], centroids) for i in range(optimal_k)}\ndf_cleaned['Cluster_Label'] = df_cleaned['Cluster'].map(cluster_labels)\n\n# Step 8: Display Final Results\nprint(\"Cluster Centroids:\")\nprint(centroids)\n\nprint(\"\\nSample Clustered Data:\")\nprint(df_cleaned[['duration', 'tempo', 'key', 'Cluster', 'Cluster_Label']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:24:09.129389Z","iopub.execute_input":"2025-02-16T14:24:09.129727Z","iopub.status.idle":"2025-02-16T14:24:11.136240Z","shell.execute_reply.started":"2025-02-16T14:24:09.129694Z","shell.execute_reply":"2025-02-16T14:24:11.135306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PCA for Visualization\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_features)\ndf_cleaned['PCA1'] = pca_result[:, 0]\ndf_cleaned['PCA2'] = pca_result[:, 1]\n\n# Map cluster labels\ncluster_label_map = df_cleaned.groupby(\"Cluster\")[\"Cluster_Label\"].first().to_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:26:14.319932Z","iopub.execute_input":"2025-02-16T14:26:14.320259Z","iopub.status.idle":"2025-02-16T14:26:14.853175Z","shell.execute_reply.started":"2025-02-16T14:26:14.320235Z","shell.execute_reply":"2025-02-16T14:26:14.852467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Clusters\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(df_cleaned['PCA1'], df_cleaned['PCA2'], c=df_cleaned['Cluster'], cmap='viridis', s=10)\n\n# Plot Cluster Centers\ncluster_centers = pca.transform(kmeans.cluster_centers_)\nplt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='X', s=100, label='Cluster Centers')\n\n# Label Cluster Centers\nfor i, (x, y) in enumerate(cluster_centers):\n    cluster_name = cluster_label_map.get(i, f\"Cluster {i}\")\n    plt.text(x, y, cluster_name, fontsize=12, fontweight='bold', ha='center', color='black',\n             bbox=dict(facecolor='white', alpha=0.6))\n\n# Final Touches\nplt.title('Clusters Visualization (PCA)')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.colorbar(scatter, label='Cluster')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:26:17.328823Z","iopub.execute_input":"2025-02-16T14:26:17.329184Z","iopub.status.idle":"2025-02-16T14:26:25.653363Z","shell.execute_reply.started":"2025-02-16T14:26:17.329155Z","shell.execute_reply":"2025-02-16T14:26:25.652384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(centroids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:27:35.775730Z","iopub.execute_input":"2025-02-16T14:27:35.776084Z","iopub.status.idle":"2025-02-16T14:27:35.783403Z","shell.execute_reply.started":"2025-02-16T14:27:35.776051Z","shell.execute_reply":"2025-02-16T14:27:35.782515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'df_cleaned' contains song features and 'kmeans.labels_' holds cluster labels\ndf_cleaned[\"mood_cluster\"] = kmeans.labels_\n\n# Save to CSV\ndf_cleaned.to_csv(\"clustered_songs.csv\", index=False)\nprint(\"Clustered dataset saved as clustered_songs.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:27:42.847463Z","iopub.execute_input":"2025-02-16T14:27:42.847764Z","iopub.status.idle":"2025-02-16T14:27:48.254150Z","shell.execute_reply.started":"2025-02-16T14:27:42.847738Z","shell.execute_reply":"2025-02-16T14:27:48.253220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Mood prediction model**","metadata":{}},{"cell_type":"code","source":"# âœ… Using the HTML-extracted DataFrame\ndf_cleaned = spotify_user_behavior.copy()\n\n# âœ… Encode Categorical Features\ncategorical_features = ['Gender', 'spotify_subscription_plan', 'preferred_listening_content', \n                         'fav_music_genre', 'music_time_slot', 'music_expl_method']\n\nlabel_encoders = {}\nfor col in categorical_features:\n    le = LabelEncoder()\n    df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n    label_encoders[col] = le\n\n# âœ… Encode the Target Variable (User Mood)\nlabel_encoder_mood = LabelEncoder()\ndf_cleaned['Encoded_Mood'] = label_encoder_mood.fit_transform(df_cleaned['music_Influencial_mood'].astype(str))\n\n# âœ… Select Features and Target\nfeatures = ['Gender', 'spotify_subscription_plan', 'preferred_listening_content', \n            'fav_music_genre', 'music_time_slot', 'music_recc_rating', 'music_expl_method']\nX = df_cleaned[features]\ny = df_cleaned['Encoded_Mood']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:27:54.225313Z","iopub.execute_input":"2025-02-16T14:27:54.225639Z","iopub.status.idle":"2025-02-16T14:27:54.237218Z","shell.execute_reply.started":"2025-02-16T14:27:54.225609Z","shell.execute_reply":"2025-02-16T14:27:54.236296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Scaling\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\n\n# âœ… Polynomial Features\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_poly = poly.fit_transform(X_scaled)\n\n# âœ… Resampling with RandomOverSampler\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X_poly, y)\n\n# âœ… Train-test Split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:04.277255Z","iopub.execute_input":"2025-02-16T14:28:04.277538Z","iopub.status.idle":"2025-02-16T14:28:04.289834Z","shell.execute_reply.started":"2025-02-16T14:28:04.277516Z","shell.execute_reply":"2025-02-16T14:28:04.289011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Logistic Regression Model with Regularization\nclass UserMoodPredictionModel:\n    def __init__(self, learning_rate=0.005, n_iterations=30000, regularization_strength=0.05):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.reg_strength = regularization_strength\n        self.weights = None\n        self.bias = None\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.classes = np.unique(y)\n        self.weights = np.zeros((len(self.classes), n_features))\n        self.bias = np.zeros(len(self.classes))\n\n        for idx, cls in enumerate(self.classes):\n            y_binary = np.where(y == cls, 1, 0)\n            for _ in range(self.n_iterations):\n                linear_model = np.dot(X, self.weights[idx]) + self.bias[idx]\n                y_predicted = self.sigmoid(linear_model)\n\n                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y_binary)) + self.reg_strength * self.weights[idx]\n                db = (1 / n_samples) * np.sum(y_predicted - y_binary)\n\n                self.weights[idx] -= self.learning_rate * dw\n                self.bias[idx] -= self.learning_rate * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights.T) + self.bias\n        y_predicted = self.sigmoid(linear_model)\n        return np.argmax(y_predicted, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:06.356793Z","iopub.execute_input":"2025-02-16T14:28:06.357167Z","iopub.status.idle":"2025-02-16T14:28:06.367075Z","shell.execute_reply.started":"2025-02-16T14:28:06.357124Z","shell.execute_reply":"2025-02-16T14:28:06.366126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Initialize and Train the Logistic Regression Model\nlogistic_model = UserMoodPredictionModel()\nlogistic_model.fit(X_train, y_train)\n\n# âœ… Random Forest Classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# âœ… Predictions\nlogistic_pred = logistic_model.predict(X_test)\nrf_pred = rf_model.predict(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:08.397365Z","iopub.execute_input":"2025-02-16T14:28:08.397763Z","iopub.status.idle":"2025-02-16T14:28:14.796311Z","shell.execute_reply.started":"2025-02-16T14:28:08.397730Z","shell.execute_reply":"2025-02-16T14:28:14.795386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Evaluation Function\ndef evaluate_model(y_true, y_pred, model_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n\n    print(f\"\\n{model_name} Performance:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n\n    unique_classes = np.unique(y_true)\n    target_names = label_encoder_mood.inverse_transform(unique_classes)\n\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, labels=unique_classes, target_names=target_names, zero_division=1))\n\n    # âœ… Confusion Matrix\n    conf_mat = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.xlabel('Predicted Mood')\n    plt.ylabel('Actual Mood')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:37.264485Z","iopub.execute_input":"2025-02-16T14:28:37.264805Z","iopub.status.idle":"2025-02-16T14:28:37.271070Z","shell.execute_reply.started":"2025-02-16T14:28:37.264780Z","shell.execute_reply":"2025-02-16T14:28:37.270121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Model Evaluation\nevaluate_model(y_test, logistic_pred, \"Logistic Regression\")\nevaluate_model(y_test, rf_pred, \"Random Forest Classifier\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:28:41.226495Z","iopub.execute_input":"2025-02-16T14:28:41.226788Z","iopub.status.idle":"2025-02-16T14:28:41.707893Z","shell.execute_reply.started":"2025-02-16T14:28:41.226766Z","shell.execute_reply":"2025-02-16T14:28:41.707067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Recommendation System Neural Network**","metadata":{}},{"cell_type":"markdown","source":"\n* The dataset is loaded and features (such as valence, danceability, etc.) and target (mood_cluster) are selected.\n* Features are normalized using StandardScaler to improve model performance.\n* The dataset is split into training and testing sets using train_test_split.\n","metadata":{}},{"cell_type":"code","source":"selected_features = [\n    'valence', 'danceability', 'energy', 'acousticness', 'tempo'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:03.815854Z","iopub.execute_input":"2025-02-16T14:29:03.816200Z","iopub.status.idle":"2025-02-16T14:29:03.820085Z","shell.execute_reply.started":"2025-02-16T14:29:03.816171Z","shell.execute_reply":"2025-02-16T14:29:03.819056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine numerical data from multiple datasets\ndef combine_numerical_data(datasets):\n    combined_data = pd.DataFrame()\n    for key, data in datasets.items():\n        if isinstance(data, dict):  # For datasets with train/test split\n            for sub_key, df in data.items():\n                if isinstance(df, pd.DataFrame):\n                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n        elif isinstance(data, pd.DataFrame):\n            combined_data = pd.concat([combined_data, data], ignore_index=True)\n    return combined_data\n\n# Load and combine datasets\nall_datasets = load_specific_datasets(data_dirs)\ncombined_data = combine_numerical_data(all_datasets)\n\n# ðŸŽ¯ Step 3: Filter for Selected Music Features\navailable_music_features = [feature for feature in selected_features if feature in combined_data.columns]\nselected_music_data = combined_data[available_music_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:04.669682Z","iopub.execute_input":"2025-02-16T14:29:04.669968Z","iopub.status.idle":"2025-02-16T14:29:14.599558Z","shell.execute_reply.started":"2025-02-16T14:29:04.669947Z","shell.execute_reply":"2025-02-16T14:29:14.598479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the clustered dataset (assuming it contains song features and mood clusters)\ndata = pd.read_csv(\"clustered_songs.csv\")  # Update with actual file path\n\n# Select features and target\nfeatures = available_music_features\ntarget = 'mood_cluster'  # Mood cluster as label\n\nX = data[features]\ny = data[target]\n\n# Normalize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:18.017062Z","iopub.execute_input":"2025-02-16T14:29:18.017387Z","iopub.status.idle":"2025-02-16T14:29:18.839072Z","shell.execute_reply.started":"2025-02-16T14:29:18.017362Z","shell.execute_reply":"2025-02-16T14:29:18.838343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* A Sequential model is used to define the neural network.\n* It has two hidden layers with ReLU activation and dropout for regularization.\n* The output layer uses a softmax activation for multi-class classification, with the number of units equal to the number of unique mood clusters\n.","metadata":{}},{"cell_type":"code","source":"# Define the neural network model\nmodel = Sequential([\n    Input(shape=(X_train.shape[1],)),  # Define input layer explicitly\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(len(y.unique()), activation='softmax')  # Output layer for classification\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:22.160556Z","iopub.execute_input":"2025-02-16T14:29:22.160856Z","iopub.status.idle":"2025-02-16T14:29:24.226249Z","shell.execute_reply.started":"2025-02-16T14:29:22.160831Z","shell.execute_reply":"2025-02-16T14:29:24.225521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* The model is compiled with Adam optimizer and sparse_categorical_crossentropy loss function.\n* The model is trained for 50 epochs with a batch size of 16, using validation data to monitor performance.\n","metadata":{}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:29:33.757666Z","iopub.execute_input":"2025-02-16T14:29:33.757957Z","iopub.status.idle":"2025-02-16T15:06:21.164232Z","shell.execute_reply.started":"2025-02-16T14:29:33.757935Z","shell.execute_reply":"2025-02-16T15:06:21.163236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Model Accuracy Over Epochs')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Model Loss Over Epochs')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:11:29.633912Z","iopub.execute_input":"2025-02-16T15:11:29.634257Z","iopub.status.idle":"2025-02-16T15:11:29.990861Z","shell.execute_reply.started":"2025-02-16T15:11:29.634229Z","shell.execute_reply":"2025-02-16T15:11:29.989931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Save the trained model\nmodel.save(\"music_mood_recommendation_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:11:37.389972Z","iopub.execute_input":"2025-02-16T15:11:37.390286Z","iopub.status.idle":"2025-02-16T15:11:41.863721Z","shell.execute_reply.started":"2025-02-16T15:11:37.390263Z","shell.execute_reply":"2025-02-16T15:11:41.862911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}